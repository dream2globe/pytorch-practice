{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handy-strength",
   "metadata": {},
   "source": [
    "# Learn about the optimization loop\n",
    "## Optimizing the model parameters\n",
    "- 모델을 학습하는 것은 반복적인 프로세스이며 epoch로 불리우는 각 반복 단계에서 모델은 출력과 에러를 계산\n",
    "- 이후에는 모듈에서의 파라미터와 관련된 에러의 미분값을 수집하고 경사하강법을 사용하여 파라미터를 최적화함\n",
    "- 이 프로세스의 자세한 내용은 [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)를 참고바람\n",
    "\n",
    "### Prerequisite code\n",
    "앞에서 소개된 코드로부터 **Dataset**, **DataLoaders**, **Build Model** 모듈이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "controlled-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "stupid-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-night",
   "metadata": {},
   "source": [
    "## Setting hyperparameters\n",
    "하이퍼파라미터는 모델 최적화 프로세스를 제어하는 조절 가능한 파라미터를 의미함. 하이퍼파라미터는 모델 학습과 수렴 속도에 영향을 줌([참고](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html))\n",
    "\n",
    "모델 훈련을 위해 다음의 하이퍼파라미터를 정의\n",
    "- **Number of Epochs** - 데이터 전체의 반복 횟수\n",
    "- **Bactch Size** - 각 epoch에서 모델에 보여지는 데이터 샘플 수 \n",
    "- **Learning Rate** - 각 batch와 epoch에서의 업데이트 크기로 작은 값는 학습 속도가 느려지지만, 큰 값은 학습 동안 예측할 수 없는 결과를 초래함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caring-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-algeria",
   "metadata": {},
   "source": [
    "## Add an optimization loop\n",
    "\n",
    "하이퍼파라미터를 설정한 다음으로 최적화 반복을 사용하여 모델 학습과 최적화할 수 있음. 최적화를 위한 각 반복을 **epoch**라 부름. 각 epoch는 두 부분으로 구성됨.\n",
    "- The Train Loop - 데이터 전체를 학습하고 최적 파라미터로 수렴을 반복적으로 시도함\n",
    "- The validation/Test Loop - 테스트 데이터 전반에서 모델 성능이 향상되었는지 확인을 반복함\n",
    "\n",
    "### Add a loss function\n",
    "손실함수는 목표값과 모델로 부터 얻은 값 사이의 차이를 측정하므로 학습하는 동안 최소화되길 원함. 손실을 계산하기 위해 주어진 데이터 샘플을 사용하여 예측한 값과 실제  값의 차이를 비교함\n",
    "\n",
    "일반적인 손실 함수는 [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error, regression), [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood, classification), [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) (combines `nn.LogSoftmax` and `nn.NLLLoss`.)\n",
    "\n",
    "출력 logit을 `nn.CrossEntropyLoss`로 전달하면, logit을 정규화하고 및 예측 에러를 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beneficial-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-scenario",
   "metadata": {},
   "source": [
    "### Optimization pass\n",
    "- 최적화는 각 학습 단계에서 모델 오차를 줄이기 위해 파라미터를 조절하는 단계\n",
    "- 모든 최적화 로직은 `optimizer` object로 캡슐레이션 되어있음\n",
    "- ADAM, RMSProp와 같은 다양한 최적화 알고리즘을 사용할 수 있음 ([different optimizers](https://pytorch.org/docs/stable/optim.html))\n",
    "- 학습률를 전달하고 학습에 필요한 파라미터를 등록함으로써 optimizer를 초기화 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "lonely-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-warren",
   "metadata": {},
   "source": [
    "반복 학습과정 내부에는 최적화가 3단계로 진행됨\n",
    "- 모델 파라미터의 경사 초기화를 위해 `optimizer.zero_grad()` 호출함. 기본값에 의해 경사는 누적됨. 이중 계산을 막기 위해 각 반복에서 명시적으로 0을 초기화해야 함\n",
    "- `loss.backwards()`을 사용하여 예측 오차를 역전파. Pytorch는 각 파라미터 손실의 경사를 저장함.\n",
    "- `optimizer.step()` 호출하면 역전파 경로에서 수집된 경사로 인해 파라미터를 조절할 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-prairie",
   "metadata": {},
   "source": [
    "### Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "capital-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-karen",
   "metadata": {},
   "source": [
    "`train_loop` 와 `test_loop`에 손실함수, 최적화 함수를 전달. 모델 성능향상을 위해 epoch 수를 높게 조절할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "developed-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300640  [    0/60000]\n",
      "loss: 2.290309  [ 6400/60000]\n",
      "loss: 2.286352  [12800/60000]\n",
      "loss: 2.284961  [19200/60000]\n",
      "loss: 2.274725  [25600/60000]\n",
      "loss: 2.273580  [32000/60000]\n",
      "loss: 2.268600  [38400/60000]\n",
      "loss: 2.267196  [44800/60000]\n",
      "loss: 2.237566  [51200/60000]\n",
      "loss: 2.240211  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.035035 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.225451  [    0/60000]\n",
      "loss: 2.211274  [ 6400/60000]\n",
      "loss: 2.202846  [12800/60000]\n",
      "loss: 2.202437  [19200/60000]\n",
      "loss: 2.185632  [25600/60000]\n",
      "loss: 2.186528  [32000/60000]\n",
      "loss: 2.178825  [38400/60000]\n",
      "loss: 2.179127  [44800/60000]\n",
      "loss: 2.110761  [51200/60000]\n",
      "loss: 2.115404  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.033081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.113491  [    0/60000]\n",
      "loss: 2.087825  [ 6400/60000]\n",
      "loss: 2.064968  [12800/60000]\n",
      "loss: 2.056619  [19200/60000]\n",
      "loss: 2.046947  [25600/60000]\n",
      "loss: 2.065896  [32000/60000]\n",
      "loss: 2.034297  [38400/60000]\n",
      "loss: 2.052040  [44800/60000]\n",
      "loss: 1.928051  [51200/60000]\n",
      "loss: 1.933435  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.030317 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.959605  [    0/60000]\n",
      "loss: 1.917577  [ 6400/60000]\n",
      "loss: 1.876417  [12800/60000]\n",
      "loss: 1.854135  [19200/60000]\n",
      "loss: 1.876710  [25600/60000]\n",
      "loss: 1.927602  [32000/60000]\n",
      "loss: 1.856583  [38400/60000]\n",
      "loss: 1.912474  [44800/60000]\n",
      "loss: 1.733532  [51200/60000]\n",
      "loss: 1.732963  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.027457 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.802685  [    0/60000]\n",
      "loss: 1.753590  [ 6400/60000]\n",
      "loss: 1.699848  [12800/60000]\n",
      "loss: 1.670871  [19200/60000]\n",
      "loss: 1.725096  [25600/60000]\n",
      "loss: 1.806880  [32000/60000]\n",
      "loss: 1.705585  [38400/60000]\n",
      "loss: 1.798454  [44800/60000]\n",
      "loss: 1.583645  [51200/60000]\n",
      "loss: 1.581472  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.025258 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.672861  [    0/60000]\n",
      "loss: 1.625977  [ 6400/60000]\n",
      "loss: 1.560068  [12800/60000]\n",
      "loss: 1.539769  [19200/60000]\n",
      "loss: 1.602993  [25600/60000]\n",
      "loss: 1.703898  [32000/60000]\n",
      "loss: 1.589475  [38400/60000]\n",
      "loss: 1.705483  [44800/60000]\n",
      "loss: 1.474349  [51200/60000]\n",
      "loss: 1.473977  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.023585 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.566365  [    0/60000]\n",
      "loss: 1.526280  [ 6400/60000]\n",
      "loss: 1.445409  [12800/60000]\n",
      "loss: 1.441497  [19200/60000]\n",
      "loss: 1.504358  [25600/60000]\n",
      "loss: 1.614698  [32000/60000]\n",
      "loss: 1.499078  [38400/60000]\n",
      "loss: 1.628716  [44800/60000]\n",
      "loss: 1.391450  [51200/60000]\n",
      "loss: 1.392760  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.022282 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.478979  [    0/60000]\n",
      "loss: 1.448136  [ 6400/60000]\n",
      "loss: 1.352083  [12800/60000]\n",
      "loss: 1.363494  [19200/60000]\n",
      "loss: 1.427865  [25600/60000]\n",
      "loss: 1.541563  [32000/60000]\n",
      "loss: 1.429166  [38400/60000]\n",
      "loss: 1.566872  [44800/60000]\n",
      "loss: 1.327564  [51200/60000]\n",
      "loss: 1.332324  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.021271 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.408407  [    0/60000]\n",
      "loss: 1.386885  [ 6400/60000]\n",
      "loss: 1.277360  [12800/60000]\n",
      "loss: 1.301832  [19200/60000]\n",
      "loss: 1.368592  [25600/60000]\n",
      "loss: 1.483817  [32000/60000]\n",
      "loss: 1.374836  [38400/60000]\n",
      "loss: 1.522137  [44800/60000]\n",
      "loss: 1.278219  [51200/60000]\n",
      "loss: 1.285619  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.020485 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.349491  [    0/60000]\n",
      "loss: 1.338654  [ 6400/60000]\n",
      "loss: 1.218173  [12800/60000]\n",
      "loss: 1.250558  [19200/60000]\n",
      "loss: 1.322092  [25600/60000]\n",
      "loss: 1.438170  [32000/60000]\n",
      "loss: 1.331827  [38400/60000]\n",
      "loss: 1.489216  [44800/60000]\n",
      "loss: 1.239552  [51200/60000]\n",
      "loss: 1.248883  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.019864 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
